{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show point clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# table of content\n",
    "1) [Showing las pointcloud](#showing-las-pointclouds)\n",
    "2) [Different file format generation](#different-file-format-generation)\n",
    "3) [Convert all files of a folder](#convert-all-files-of-folder)\n",
    "4) [Semantic splitting](#semantic-splitting)\n",
    "5) [Instance splitting](#instances-splitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies and general utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import open3d as o3d\n",
    "import laspy\n",
    "import pdal\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing las pointclouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_laz(file_path):\n",
    "    las = laspy.read(file_path)\n",
    "    points = np.vstack((las.x, las.y, las.z)).transpose()  # Extract XYZ coordinates\n",
    "    return points\n",
    "\n",
    "def print_camera_info(vis):\n",
    "    ctr = vis.get_view_control()\n",
    "\n",
    "    saved_cam_params = ctr.convert_to_pinhole_camera_parameters()\n",
    "\n",
    "    # Extract parameters\n",
    "    intrinsic = saved_cam_params.intrinsic.intrinsic_matrix\n",
    "    extrinsic = saved_cam_params.extrinsic\n",
    "    fov = ctr.get_field_of_view()\n",
    "\n",
    "    print(\"\\nCamera Parameters:\")\n",
    "    print(f\"Intrinsic Matrix:\\n{intrinsic}\")\n",
    "    print(f\"Extrinsic Matrix:\\n{extrinsic}\")\n",
    "    print(f\"Field of View: {fov} degrees\\n\")\n",
    "    return False  # Do not block the visualization loop\n",
    "\n",
    "\n",
    "def apply_camera_params(vis, extrinsic):\n",
    "    ctr = vis.get_view_control()\n",
    "\n",
    "    # Convert to Open3D format\n",
    "    cam_params = ctr.convert_to_pinhole_camera_parameters()\n",
    "    # cam_params.intrinsic.set_intrinsics(1280, 720, intrinsic[0][0], intrinsic[1][1], intrinsic[0][2], intrinsic[1][2])\n",
    "    cam_params.extrinsic = np.array(extrinsic)\n",
    "\n",
    "    # Apply settings\n",
    "    ctr.convert_from_pinhole_camera_parameters(cam_params, allow_arbitrary=True)\n",
    "\n",
    "    return False\n",
    "\n",
    "def set_orthographic_view(vis, zoom=0.5, front=[0, 0, -1], lookat=[0, 0, 0], up=[0, -1, 0]):\n",
    "    ctr = vis.get_view_control()\n",
    "    ctr.set_lookat(lookat)\n",
    "    ctr.set_front(front)\n",
    "    ctr.set_up(up)\n",
    "    ctr.set_zoom(zoom)\n",
    "\n",
    "def visualize_laz(file_path, mode='normal', display_mode='3d', extrinsic=None, projection=\"orthographic\", image_dest_src=None, gt=False):\n",
    "    # Load point cloud\n",
    "    # pcd = o3d.io.read_point_cloud(file_path)  # Replace with your file\n",
    "\n",
    "    # assert not semantic == instance == True\n",
    "    assert mode in ['normal', 'instance', 'semantic']\n",
    "    assert display_mode in ['3d', 'image']\n",
    "\n",
    "    laz = laspy.read(file_path)\n",
    "    points = load_laz(file_path)\n",
    " \n",
    "    if mode == 'semantic':\n",
    "        cat = getattr(laz, 'PredSemantic') if gt == False else getattr(laz, 'gt_semantic_segmentation')\n",
    "        colors = np.zeros((cat.size, 3))\n",
    "        colors[np.arange(cat.size), cat] = 1\n",
    "\n",
    "    if mode == 'instance':\n",
    "        # list of colors\n",
    "        list_colors = [\n",
    "            (1.00, 0.00, 0.00), (0.00, 1.00, 0.00), (0.00, 0.00, 1.00), (1.00, 1.00, 0.00), (1.00, 0.00, 1.00),\n",
    "            (0.00, 1.00, 1.00), (0.50, 0.00, 0.00), (0.50, 0.50, 0.00), (0.00, 0.50, 0.00), (0.50, 0.00, 0.50),\n",
    "            (0.00, 0.50, 0.50), (0.00, 0.00, 0.50), (1.00, 0.65, 0.00), (0.65, 0.16, 0.16), (0.54, 0.17, 0.89),\n",
    "            (0.37, 0.62, 0.63), (0.50, 1.00, 0.00), (0.86, 0.08, 0.24), (1.00, 0.84, 0.00), (0.29, 0.00, 0.51)\n",
    "        ]\n",
    "\n",
    "        # cat = getattr(laz, 'PredInstance') if gt == False else getattr(laz, 'gt_instance_segmentation')\n",
    "        cat = getattr(laz, 'PredInstance') if gt == False else np.array(getattr(laz, 'treeID'))\n",
    "        print(cat)\n",
    "        colors = np.zeros((cat.size, 3))\n",
    "        for i in range(cat.size):\n",
    "            colors[i, :] = list_colors[cat[i] % len(list_colors)]\n",
    "        \n",
    "\n",
    "    # Convert numpy array to Open3D point cloud\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "\n",
    "    if mode in ['semantic', 'instance']:\n",
    "        pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "        \n",
    "    # Create visualizer\n",
    "    vis = o3d.visualization.VisualizerWithKeyCallback()\n",
    "    vis.create_window()\n",
    "    vis.add_geometry(pcd)\n",
    "\n",
    "    if projection == 'orthographic':\n",
    "        bbox = pcd.get_axis_aligned_bounding_box()\n",
    "        center = bbox.get_center()\n",
    "        extent = bbox.get_extent()\n",
    "        largest_dim = max(extent)\n",
    "        set_orthographic_view(\n",
    "            vis,\n",
    "            zoom=1.0 / (largest_dim * 0.01),\n",
    "            front=[0, 0, -1],\n",
    "            lookat=center,\n",
    "            up=[0, -1, 0]\n",
    "        )\n",
    "        \n",
    "    if extrinsic:\n",
    "        apply_camera_params(vis, extrinsic)\n",
    "\n",
    "    if display_mode == '3d':\n",
    "        # Assign 'C' key (67 in ASCII) to print camera vectors\n",
    "        vis.register_key_callback(67, print_camera_info)  # Press 'C' to print vectors\n",
    "\n",
    "        # Run visualization\n",
    "        vis.run()\n",
    "        vis.destroy_window()\n",
    "    else:\n",
    "        # ðŸ”¹ Step 3: Capture rendered image\n",
    "        image = vis.capture_screen_float_buffer(do_render=True)  # Get image as a NumPy array\n",
    "        vis.destroy_window()  # Close the visualizer\n",
    "\n",
    "        # ðŸ”¹ Step 4: Convert and show image with Matplotlib\n",
    "        fig = plt.figure(figsize=(10,10))\n",
    "        image_np = np.asarray(image)  # Convert to NumPy array\n",
    "        plt.imshow(image_np)\n",
    "        plt.axis(\"off\")  # Hide axes\n",
    "        plt.show()\n",
    "        if image_dest_src:\n",
    "            plt.imsave(image_dest_src, image_np)\n",
    "\n",
    "# Convert to Open3D PointCloud and visualize\n",
    "def visualize_laz_base(file_path, segmented = False):\n",
    "    laz = laspy.read(file_path)\n",
    "    points = load_laz(file_path)\n",
    "\n",
    "    if segmented:\n",
    "        cat = getattr(laz, 'PredSemantic')\n",
    "        colors = np.zeros((cat.size, 3))\n",
    "        colors[np.arange(cat.size), cat] = 1\n",
    "    \n",
    "    # Convert numpy array to Open3D point cloud\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "\n",
    "    if segmented:\n",
    "        pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "    o3d.visualization.draw_geometries([pcd],\n",
    "                                  up=[-0.01603346, -0.87699978, -0.48022319],\n",
    "                                  front=[-0.15798422, -0.47202973,  0.86731132],\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declare sample paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src_test_file = \"../data/testing_samples/split_0332.laz\"\n",
    "src_test_file = r\"D:\\PDM_repo\\Github\\PDM\\data\\gt\\color_grp_full_tile_316_gt.laz\"\n",
    "# src_test_file = r\"D:\\PDM_repo\\FORinstance_dataset\\SCION\\plot_39_annotated.las\"\n",
    "# src_test_file = \"../data/split_testing/tiles/color_grp_full_tile_32.laz\"\n",
    "# src_test_file_segmented = \"../data/testing_samples/split_0332_out.laz\"\n",
    "src_test_file_segmented = r\"D:\\PDM_repo\\Github\\PDM\\data\\gt\\color_grp_full_tile_316_gt.laz\"\n",
    "# file = laspy.read(src_test_file)\n",
    "# print(list(file.point_format.dimension_names))\n",
    "# # print(len(list(file.classification)))\n",
    "# for val in set(file.treeID):\n",
    "#     print(val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show different versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize original file\n",
    "src_out_image = src_test_file.split('.laz')[0] + '_original.png'\n",
    "matrix = [\n",
    "    [ 9.26545534e-01,  3.74424345e-01, -3.63288108e-02, -2.54248968e+06],\n",
    "    [ 1.92236310e-01, -5.54279499e-01, -8.09826795e-01,  3.01249829e+06],\n",
    "    [-3.23355183e-01,  7.43357684e-01, -5.85543149e-01, -4.00801631e+06],\n",
    "    [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.00000000e+00]\n",
    "]\n",
    "matrix = [\n",
    "    [0.799004256, -0.562297342, 0.213105367, -1437610.28],\n",
    "    [0.0222251102, -0.326535956, -0.944923443, 301431.209],\n",
    "    [0.600914505, 0.759734144, -0.248406498, -2379796.27],\n",
    "    [0.0, 0.0, 0.0, 1.0]\n",
    "]\n",
    "visualize_laz(src_test_file, mode='normal', display_mode='3d', extrinsic=None, image_dest_src=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize original file with preset camera pos\n",
    "# declare camera state matrices and apply them\n",
    "extrinsic = [\n",
    "        [ 8.91845328e-01, -3.93401864e-01,  2.23264158e-01, -1.86350146e+06],\n",
    "        [ 7.94960049e-02, -3.49579457e-01, -9.33528033e-01,  1.80762530e+05],\n",
    "        [ 4.45300231e-01,  8.50311224e-01, -2.80496929e-01, -2.07756524e+06],\n",
    "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.00000000e+00]\n",
    "    ]\n",
    "src_preset_file = \"../data/split_testing/tiles/color_grp_full_tile_311.laz\"\n",
    "src_out_image = src_test_file.split('.laz')[0] + '_original.png'\n",
    "visualize_laz(src_preset_file, mode='normal', display_mode='image', extrinsic=extrinsic, image_dest_src=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize semantic segmentation\n",
    "\n",
    "src_out_image = src_test_file_segmented.split('_out.laz')[0] + '_semantic.png'\n",
    "visualize_laz(src_test_file_segmented, mode='semantic', display_mode='3d', image_dest_src=None, gt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize instance segmentation\n",
    "src_out_image = src_test_file_segmented.split('_out.laz')[0] + '_instance.png'\n",
    "visualize_laz(src_test_file, mode='instance', display_mode='3d', image_dest_src=None, gt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different file format generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_laz_to_las(in_laz, out_las, verbose=True):\n",
    "    las = laspy.read(in_laz)\n",
    "    las = laspy.convert(las)\n",
    "    las.write(out_las)\n",
    "    if verbose:\n",
    "        print(f\"LAZ file saved in {out_las}\")\n",
    "\n",
    "def convert_las_to_laz(in_las, out_laz, verbose=True):\n",
    "    \"\"\"\n",
    "    Convert a LAS file to a LAZ file, stripping all extra dimensions.\n",
    "\n",
    "    Parameters:\n",
    "    - in_las: str, path to the input .las file\n",
    "    - out_laz: str, path to the output .laz file\n",
    "    - verbose: bool, whether to print a success message\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    pipeline_json = {\n",
    "        \"pipeline\": [\n",
    "            {\n",
    "                \"type\": \"readers.las\",\n",
    "                \"filename\": in_las\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"writers.las\",\n",
    "                \"filename\": out_laz,\n",
    "                \"compression\": \"laszip\",  # Ensure compression to LAZ\n",
    "                # \"extra_dims\": \"none\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Create and execute the pipeline\n",
    "    pipeline = pdal.Pipeline(json.dumps(pipeline_json))\n",
    "    pipeline.execute()\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"LAZ file saved at {out_laz}\")\n",
    "\n",
    "def convert_pcd_to_laz(in_pcd, out_laz, verbose=True):\n",
    "    # pcd = laspy.read('../data/testing_samples/split_0332.pcd')\n",
    "    pipeline_json = {\n",
    "        \"pipeline\": [\n",
    "            in_pcd,  # Read the PCD file\n",
    "            {\n",
    "                \"type\": \"writers.las\",\n",
    "                \"filename\": out_laz,\n",
    "                \"compression\": \"laszip\"  # Ensures .laz compression\n",
    "                \"\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"filters.reprojection\",\n",
    "                \"in_srs\": \"EPSG:4326\",\n",
    "                \"out_srs\": \"EPSG:2056\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Run the PDAL pipeline\n",
    "    pipeline = pdal.Pipeline(json.dumps(pipeline_json))\n",
    "    pipeline.execute()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"LAZ file saved in {out_laz}\")\n",
    "\n",
    "def convert_laz_to_pcd(in_laz, out_pcd, verbose=True):\n",
    "    laz = laspy.read(in_laz)\n",
    "\n",
    "    # Gathering all attributes from laz file\n",
    "    points = np.vstack((laz.x, laz.y, laz.z)).T\n",
    "\n",
    "    attributes = {}\n",
    "    for attribute in laz.point_format.dimensions:\n",
    "        if attribute.name in ['X', 'Y', 'Z']:\n",
    "            continue\n",
    "        attributes[attribute.name] = getattr(laz, attribute.name)\n",
    "    \n",
    "    # Preparing data for pcd\n",
    "    num_points = points.shape[0]\n",
    "    fields = [\"x\", \"y\", \"z\"] + list(attributes.keys())  # All field names\n",
    "    types = [\"F\", \"F\", \"F\"] + [\"F\" for _ in attributes]  # Float32 fields\n",
    "    sizes = [4] * len(fields)  # 4-byte float per field\n",
    "\n",
    "    # Stack all data into a single NumPy array\n",
    "    data = np.column_stack([points] + [attributes[key] for key in attributes])\n",
    "\n",
    "    # Write to a PCD file\n",
    "    with open(out_pcd, \"w\") as f:\n",
    "        # f.write(f\"# .PCD v0.7 - Point Cloud Data file format\\n\")\n",
    "        f.write(f\"VERSION 0.7\\n\")\n",
    "        f.write(f\"FIELDS {' '.join(fields)}\\n\")\n",
    "        f.write(f\"SIZE {' '.join(map(str, sizes))}\\n\")\n",
    "        f.write(f\"TYPE {' '.join(types)}\\n\")\n",
    "        f.write(f\"COUNT {' '.join(['1'] * len(fields))}\\n\")\n",
    "        f.write(f\"WIDTH {num_points}\\n\")\n",
    "        f.write(f\"HEIGHT 1\\n\")\n",
    "        f.write(f\"VIEWPOINT 0 0 0 1 0 0 0\\n\")\n",
    "        f.write(f\"POINTS {num_points}\\n\")\n",
    "        f.write(f\"DATA ascii\\n\")\n",
    "    \n",
    "        # Write data\n",
    "        np.savetxt(f, data, fmt=\" \".join([\"%.6f\"] * len(fields)))\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"PCD file saved in {out_pcd}\")\n",
    "\n",
    "\n",
    "# convert_pcd_to_laz(r\"C:\\temp_stockage_pdm\\PDM_repos\\Data_samples_cat\\Single\\color_grp_000020.pcd\",r\"C:\\temp_stockage_pdm\\PDM_repos\\Data_samples_cat\\Single\\color_grp_000020.laz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform laz to las and pcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From laz to las\n",
    "for file_in in [src_test_file, src_test_file_segmented]:\n",
    "    file_out = file_in.split('.laz')[0] + '.las'\n",
    "    convert_laz_to_las(file_in, file_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From laz to pcd\n",
    "for file_in in [src_test_file, src_test_file_segmented]:\n",
    "    file_out = file_in.split('.laz')[0] + '.pcd'\n",
    "    convert_laz_to_pcd(file_in, file_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert all files of folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### from laz to pcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src_folder = r\"..\\data\\split_testing\\gt\\corrections\\color_grp_full_tile_633_out_split_instance\\missed\"\n",
    "src_folder = r\"D:\\PDM_repo\\Data_full_pc\\full_pc_flattened_barycentric\\Flattened_PC\\groups_las\"\n",
    "src_folder = r\"D:\\PDM_repo\\Github\\PDM\\data\\full_dataset\\selection\\clusters_4\\cluster_2\\gt\\round1\"\n",
    "files = [x for x in os.listdir(src_folder) if x.endswith('.laz')]\n",
    "for _, file in tqdm(enumerate(files), total=len(files)):\n",
    "    file_out = file.split('.laz')[0] + '.pcd'\n",
    "    convert_laz_to_pcd(os.path.join(src_folder, file), os.path.join(src_folder, file_out), verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### from pcd to laz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_folder = r\"..\\data\\split_testing\\gt\\final_gt\\color_grp_full_tile_633_final\\instances\"\n",
    "src_folder = r\"D:\\PDM_repo\\Github\\PDM\\data\\dataset_pipeline\\tiles_20\\loops - Copy\\0\\preds\\color_grp_full_tile_124_out_split_instance\\results\\single\"\n",
    "list_files = os.listdir(src_folder)\n",
    "for _, file in tqdm(enumerate([x for x in list_files if x.endswith('.pcd')]), total=len([x for x in list_files if x.endswith('.pcd')])):\n",
    "    if file.endswith('.pcd'):\n",
    "        src_in = os.path.join(src_folder, file)\n",
    "        src_out = os.path.join(src_folder, file.split('.pcd')[0] + '.laz')\n",
    "        convert_pcd_to_laz(src_in, src_out,verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### from las to laz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src_folder = r\"..\\data\\split_testing\\gt\\corrections\\color_grp_full_tile_633_out_split_instance\\missed\"\n",
    "src_folder = r\"D:\\PDM_repo\\Github\\PDM\\data\\full_dataset\\selection\\clusters_4\\cluster_2\\gt\\round2\"\n",
    "files = [x for x in os.listdir(src_folder) if x.endswith('.las')]\n",
    "for _, file in tqdm(enumerate(files), total=len(files)):\n",
    "    file_out = file.split('.las')[0] + '.laz'\n",
    "    convert_las_to_laz(os.path.join(src_folder, file), os.path.join(src_folder, file_out), verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_semantic(src, show_images=False, verbose=True):\n",
    "    # Define target folder:\n",
    "    dir_target = os.path.dirname(src) + '/' + src.split('/')[-1].split('.')[0] + \"_split_semantic\"\n",
    "\n",
    "    if not os.path.exists(dir_target):\n",
    "        os.makedirs(dir_target)\n",
    "\n",
    "    points_segmented = laspy.read(src_test_file_segmented)\n",
    "    val_to_name = ['ground', 'tree']\n",
    "\n",
    "    for val, name in enumerate(val_to_name):\n",
    "        file_name = src_test_file_segmented.split('\\\\')[-1].split('/')[-1].split('.laz')[0] + f'_{name}.laz'\n",
    "        file_src = os.path.join(dir_target, file_name)\n",
    "\n",
    "        # create new file\n",
    "        # new_file = laspy.create(point_format=points_segmented.header.point_format, file_version=points_segmented.header.version)\n",
    "        # new_file.points = points_segmented.points[points_segmented.PredSemantic == val]\n",
    "        # new_file.write(file_src, do_compress=True)\n",
    "\n",
    "        # Define the PDAL pipeline for filtering\n",
    "        pipeline_json = {\n",
    "            \"pipeline\": [\n",
    "                src,\n",
    "                {\n",
    "                    \"type\": \"filters.expression\",\n",
    "                    \"expression\": f\"PredSemantic == {val}\"\n",
    "                },\n",
    "                file_src\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        # Run PDAL pipeline\n",
    "        pipeline = pdal.Pipeline(json.dumps(pipeline_json))\n",
    "        pipeline.execute()\n",
    "        \n",
    "        # convert new file\n",
    "        convert_laz_to_las(file_src, file_src.split('.laz')[0] + '.las', verbose=verbose)\n",
    "        convert_laz_to_pcd(file_src, file_src.split('.laz')[0] + '.pcd', verbose=verbose)\n",
    "        if show_images:\n",
    "            visualize_laz(file_src, mode='normal', display_mode='image')\n",
    "    if verbose:\n",
    "        print(\"SEMANTIC SPLITTING DONE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_semantic(src_test_file_segmented, show_images=True, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instances splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_instance(src, show_images=False, instance_col_name='PredInstance', verbose=True):\n",
    "    # Define target folder:\n",
    "    dir_target = os.path.dirname(src) + '/' + os.path.basename(src).split('/')[-1].split('.laz')[0] + \"_split_instance\"\n",
    "\n",
    "    if not os.path.exists(dir_target):\n",
    "        os.makedirs(dir_target)\n",
    "\n",
    "    points_segmented = laspy.read(src)\n",
    "\n",
    "    for idx, instance in tqdm(enumerate(set(points_segmented[instance_col_name])), total=len(set(points_segmented[instance_col_name]))):\n",
    "        file_name = src.split('\\\\')[-1].split('/')[-1].split('.laz')[0] + f'_{instance}.laz'\n",
    "        file_src = os.path.join(dir_target, file_name)\n",
    "\n",
    "        # create new file\n",
    "        # new_file = laspy.create(point_format=points_segmented.header.point_format, file_version=points_segmented.header.version)\n",
    "        # new_file.points = points_segmented.points[points_segmented.PredInstance == instance]\n",
    "        # # print(list(new_file.point_format.dimension_names))\n",
    "        # # print(new_file.PredInstance)\n",
    "        # # continue\n",
    "        # new_file.write(file_src)\n",
    "\n",
    "        # Define the PDAL pipeline for filtering\n",
    "        pipeline_json = {\n",
    "            \"pipeline\": [\n",
    "                src,\n",
    "                {\n",
    "                    \"type\": \"filters.expression\",\n",
    "                    \"expression\": f\"{instance_col_name} == {instance}\"\n",
    "                },\n",
    "                file_src\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        # Run PDAL pipeline\n",
    "        pipeline = pdal.Pipeline(json.dumps(pipeline_json))\n",
    "        pipeline.execute()\n",
    "\n",
    "        # convert new file\n",
    "        convert_laz_to_las(file_src, file_src.split('.laz')[0] + '.las', verbose=False)\n",
    "        convert_laz_to_pcd(file_src, file_src.split('.laz')[0] + '.pcd', verbose=False)\n",
    "        if show_images:\n",
    "            visualize_laz(file_src, mode='normal', display_mode='image')\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"INSTANCE SPLITTING DONE on {src}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_file = r\"D:\\PDM_repo\\Github\\PDM\\results\\for_paper\\final\\20250701_100932_eval_final_gt\\9\\pseudo_labels\\test\\color_grp_full_tile_317_gt.laz\"\n",
    "split_instance(src_file, show_images=False, instance_col_name='treeID', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting all files of one folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_folder = \"../data/classification_gt\"\n",
    "files = [x for x in os.listdir(src_folder) if x.endswith('.laz')]\n",
    "for file in files:\n",
    "    split_instance(os.path.join(src_folder, file), instance_col_name='gt_instance_segmentation', show_images=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RANDOM STUFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_test_file = r\"D:\\PDM_repo\\Github\\PDM\\data\\temp\\group_3000010.las\"\n",
    "src_test_file = r\"D:\\PDM_repo\\Github\\PDM\\data\\temp\\group_3008954.las\"\n",
    "src_test_file = r\"D:\\PDM_repo\\Github\\PDM\\data\\temp\\group_3009484.las\"\n",
    "src_test_file = r\"D:\\PDM_repo\\Github\\PDM\\data\\temp\\group_3012715.las\"\n",
    "# src_test_file = r\"D:\\PDM_repo\\Github\\PDM\\data\\temp\\group_11002366.laz\"\n",
    "points = laspy.read(src_test_file)\n",
    "# points_segmented = laspy.read(src_test_file_segmented)\n",
    "\n",
    "# List available attributes\n",
    "print(\"Available point attributes in original file:\")\n",
    "for x in list(points.point_format.dimension_names):\n",
    "    print(x)\n",
    "print('---')\n",
    "print(\"Extra dimension names:\")\n",
    "for ed in points.point_format.extra_dimension_names:\n",
    "    print(ed)\n",
    "print('---')\n",
    "print(\"set of values of 'classification': \", set(points.classification))\n",
    "print(\"set of values of 'point_source_id': \", set(points.point_source_id))\n",
    "print(\"First element of ExtraBytes: \", points['ExtraBytes'][0])\n",
    "# print(\"Available point attributes in segmented file:\")\n",
    "# print(list(points_segmented.point_format.dimension_names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(points_segmented.PredSemantic))\n",
    "for val in set(points_segmented.PredSemantic):\n",
    "    print(f\"Number of point of class {val}: {len([x for x in points_segmented.PredSemantic if x == val])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_laz(src_test_file_segmented, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### finding the average number of points / m^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = laspy.read(r\"D:\\PDM_repo\\Data_full_pc\\full_pc_color_grp\\color_grp_full.laz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min = full_data.x.min()\n",
    "x_max = full_data.x.max()\n",
    "y_min = full_data.y.min()\n",
    "y_max = full_data.y.max()\n",
    "x_length = x_max - x_min\n",
    "y_length = y_max - y_min\n",
    "surf = x_length * y_length\n",
    "num_points = len(full_data)\n",
    "density = num_points / surf\n",
    "print(\"X span [m]: \", x_length)\n",
    "print(\"Y span [m]: \", y_length)\n",
    "print(\"Surface: [m^2]\", surf)\n",
    "print(\"Number of points [-]: \", num_points)\n",
    "print(\"Density of points [m^-2]: \", round(density, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudo-labels evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_labels_layer(data_folder, tile_name, loops=[0,1,3,5,9], show_figure=True, save_figure=False):\n",
    "    # security\n",
    "    for loop in loops:\n",
    "        assert os.path.exists(os.path.join(data_folder, str(loop)))\n",
    "\n",
    "    os.makedirs(os.path.join(data_folder, 'pseudo_labels_evolution'), exist_ok=True)\n",
    "    old_max = 0\n",
    "    init=0\n",
    "    for loop in loops:\n",
    "        loop_tile_src = os.path.join(data_folder, str(loop), 'pseudo_labels', tile_name)\n",
    "        tile = laspy.read(loop_tile_src)\n",
    "        new_dim = np.zeros_like(tile.treeID)\n",
    "        if loop == 0:\n",
    "            new_dim[tile.treeID > 0] = 1\n",
    "            new_dim[tile.treeID > old_max] = 2\n",
    "            old_max = max(tile.treeID)\n",
    "            init = max(tile.treeID)\n",
    "        else:\n",
    "            new_dim[(tile.treeID > 0) & (tile.treeID <= init)] = 1\n",
    "            new_dim[(tile.treeID > init) & (tile.treeID <= old_max)] = 2\n",
    "            new_dim[tile.treeID > old_max] = 3\n",
    "            old_max = max(tile.treeID)\n",
    "\n",
    "        # add new dim\n",
    "        new_trees = laspy.ExtraBytesParams(name=\"new_trees\", type=np.uint8)\n",
    "        tile.add_extra_dim(new_trees)\n",
    "        tile['new_trees'] = new_dim\n",
    "        tile.write(os.path.join(data_folder, 'pseudo_labels_evolution', tile_name.split('laz')[0] + f'_loop_{loop}.laz'))\n",
    "\n",
    "data_folder = r\"D:\\PDM_repo\\Github\\PDM\\results\\for_paper\\final\\20250701_162429_final_on_gt\"\n",
    "tile_name = \"color_grp_full_tile_317_gt.laz\"\n",
    "add_new_labels_layer(data_folder, tile_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proportion of gt evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_segmentation_for_PQ(instances):\n",
    "    formated = []\n",
    "    # preds_format = []\n",
    "    \n",
    "    # Computing instances\n",
    "    for instance in set(instances):\n",
    "        if instance == 0: continue\n",
    "        list_points = [pos for pos, val in enumerate(instances) if val == instance]\n",
    "        formated.append(set(list_points))\n",
    "\n",
    "    # for instance in set(instance_pred):\n",
    "    #     if instance == 0: continue\n",
    "    #     list_points = [pos for pos, val in enumerate(instance_pred) if val == instance]\n",
    "    #     preds_format.append(set(list_points))\n",
    "\n",
    "    # Computing semantic\n",
    "    # for semantic in set(semantic_list):\n",
    "    #     list_points = [pos for pos, val in enumerate(semantic_list) if val == semantic]\n",
    "    #     semantic_format.append(set(list_points))\n",
    "\n",
    "    return formated\n",
    "\n",
    "\n",
    "\n",
    "def remove_duplicates(laz_file, decimals=2):\n",
    "    coords = np.round(np.vstack((laz_file.x, laz_file.y, laz_file.z)).T, decimals)\n",
    "    _, unique_indices = np.unique(coords, axis=0, return_index=True)\n",
    "    mask = np.zeros(len(coords), dtype=bool)\n",
    "    mask[unique_indices] = True\n",
    "\n",
    "    # Create new LAS object\n",
    "    header = laspy.LasHeader(point_format=laz_file.header.point_format, version=laz_file.header.version)\n",
    "    new_las = laspy.LasData(header)\n",
    "\n",
    "    setattr(new_las, 'x', np.array(laz_file.x)[mask])\n",
    "    setattr(new_las, 'y', np.array(laz_file.y)[mask])\n",
    "    setattr(new_las, 'z', np.array(laz_file.z)[mask])\n",
    "    for dim in [x for x in laz_file.point_format.dimension_names if x not in ['X', 'Y', 'Z']]:\n",
    "        setattr(new_las, dim, np.array(laz_file[dim])[mask])\n",
    "\n",
    "    return new_las\n",
    "    \n",
    "\n",
    "def match_pointclouds(laz1, laz2):\n",
    "    \"\"\"Sort laz2 to match the order of laz1 without changing laz1's order.\n",
    "\n",
    "    Args:\n",
    "        laz1: laspy.LasData object (reference order)\n",
    "        laz2: laspy.LasData object (to be sorted)\n",
    "    \n",
    "    Returns:\n",
    "        laz2 sorted to match laz1\n",
    "    \"\"\"\n",
    "    # Retrieve and round coordinates for robust matching\n",
    "    coords_1 = np.round(np.vstack((laz1.x, laz1.y, laz1.z)), 2).T\n",
    "    coords_2 = np.round(np.vstack((laz2.x, laz2.y, laz2.z)), 2).T\n",
    "\n",
    "    # Verify laz2 is of the same size as laz1\n",
    "    assert len(coords_2) == len(coords_1), \"laz2 should be a subset of laz1\"\n",
    "\n",
    "    # Create a dictionary mapping from coordinates to indices\n",
    "    coord_to_idx = {tuple(coord): idx for idx, coord in enumerate(coords_1)}\n",
    "\n",
    "    # Find indices in laz1 that correspond to laz2\n",
    "    matching_indices = []\n",
    "    failed = 0\n",
    "    for coord in coords_2:\n",
    "        try:\n",
    "            matching_indices.append(coord_to_idx[tuple(coord)])\n",
    "        except Exception as e:\n",
    "            failed += 1\n",
    "\n",
    "    matching_indices = np.array([coord_to_idx[tuple(coord)] for coord in coords_2])\n",
    "\n",
    "    # Sort laz2 to match laz1\n",
    "    sorted_indices = np.argsort(matching_indices)\n",
    "\n",
    "    # Apply sorting to all attributes of laz2\n",
    "    laz2.points = laz2.points[sorted_indices]\n",
    "\n",
    "    return laz2  # Now sorted to match laz1\n",
    "\n",
    "\n",
    "def compute_iou(mask1: np.ndarray, mask2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute Intersection over Union (IoU) between two binary masks.\n",
    "\n",
    "    mask1: First binary mask (0s and 1s)\n",
    "    mask2: Second binary mask (0s and 1s)\n",
    "    return: IoU value (float between 0 and 1)\n",
    "    \"\"\"\n",
    "    intersection = np.logical_and(mask1, mask2).sum()\n",
    "    union = np.logical_or(mask1, mask2).sum()\n",
    "    if union == 0:\n",
    "        return 1.0 if intersection == 0 else 0.0  # Handle edge case\n",
    "    return intersection / union\n",
    "\n",
    "\n",
    "def compute_frac_of_gt_found(gt_instances, pred_instances, th=0.2):\n",
    "    num_matches = 0\n",
    "    for i, gt in tqdm(enumerate(gt_instances), total=len(gt_instances), desc=\"Processing\", disable=True):\n",
    "        for j, pred in enumerate(pred_instances):\n",
    "            iou = len(gt & pred) / len(gt | pred)  # IoU computation\n",
    "            if iou > th:\n",
    "                num_matches += 1\n",
    "                break\n",
    "\n",
    "    return num_matches, num_matches / len(gt_instances)\n",
    "\n",
    "def find_extra_trees(gt_instances, pred_instances, th=0.2):\n",
    "    num_extra = 0\n",
    "    lst_extra_trees = []\n",
    "    for i, pred in tqdm(enumerate(pred_instances), total=len(pred_instances), desc=\"Processing\", disable=True):\n",
    "        if pred == 0:\n",
    "            continue\n",
    "        new_tree = True\n",
    "        for j, gt in enumerate(gt_instances):\n",
    "            iou = len(gt & pred) / len(gt | pred)  # IoU computation\n",
    "            if iou > th:\n",
    "                new_tree = False\n",
    "                break\n",
    "        if new_tree:\n",
    "            num_extra += 1\n",
    "            lst_extra_trees.append(i)\n",
    "\n",
    "    return num_extra, num_extra/len(gt_instances), lst_extra_trees\n",
    "\n",
    "\n",
    "# # test\n",
    "# tile_original = laspy.read(r\"D:\\PDM_repo\\Github\\PDM\\results\\for_paper\\final\\20250701_162429_final_on_gt\\pseudo_labels\\color_grp_full_tile_317_gt.laz\")\n",
    "# tile_preds = laspy.read(r\"D:\\PDM_repo\\Github\\PDM\\results\\for_paper\\final\\20250701_162429_final_on_gt\\0\\preds\\color_grp_full_tile_317_gt_out.laz\")\n",
    "\n",
    "# # match the original with the pred\n",
    "# tile_original = remove_duplicates(tile_original)\n",
    "# tile_preds = remove_duplicates(tile_preds)\n",
    "# match_pointclouds(tile_original, tile_preds)\n",
    "# gt_instances = tile_original.gt_instance\n",
    "# pred_instances = tile_preds.PredInstance\n",
    "# gt_instances = format_segmentation_for_PQ(gt_instances)\n",
    "# pred_instances = format_segmentation_for_PQ(pred_instances)\n",
    "# num_extra, lst_extra = find_extra_trees(gt_instances, pred_instances)\n",
    "# print(num_extra)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce csv file with those results\n",
    "src_folder = r\"D:\\PDM_repo\\Github\\PDM\\results\\for_paper\\final\\20250701_162429_final_on_gt\"\n",
    "tile_name = \"color_grp_full_tile_317_gt.laz\"\n",
    "\n",
    "lst_loops = []\n",
    "num_loop = 0\n",
    "while True:\n",
    "    if not str(num_loop) in os.listdir(src_folder):\n",
    "        break\n",
    "    lst_loops.append(num_loop)\n",
    "    num_loop += 1\n",
    "if num_loop == 0:\n",
    "    print(\"No loop folder from which to extract the pseudo-labels\")\n",
    "\n",
    "lst_absolute_val = []\n",
    "lst_frac = []\n",
    "lst_num_extra_trees = []\n",
    "lst_num_extra_trees_frac = []\n",
    "lst_lst_extra_trees = []\n",
    "tile_gt = laspy.read(os.path.join(src_folder, 'pseudo_labels', tile_name))\n",
    "tile_gt = remove_duplicates(tile_gt)\n",
    "gt_instances = tile_gt.gt_instance\n",
    "gt_instances = format_segmentation_for_PQ(gt_instances)\n",
    "for _, loop in tqdm(enumerate(lst_loops), total=len(lst_loops), desc=\"Processing\"):\n",
    "    # tile_original = laspy.read(r\"D:\\PDM_repo\\Github\\PDM\\results\\for_paper\\final\\20250701_162429_final_on_gt\\pseudo_labels\\color_grp_full_tile_317_gt.laz\")\n",
    "    # tile_preds = laspy.read(r\"D:\\PDM_repo\\Github\\PDM\\results\\for_paper\\final\\20250701_162429_final_on_gt\\0\\preds\\color_grp_full_tile_317_gt_out.laz\")\n",
    "    tile_preds = laspy.read(os.path.join(src_folder, str(loop),'pseudo_labels', tile_name))\n",
    "\n",
    "    # match the original with the pred\n",
    "    tile_preds = remove_duplicates(tile_preds)\n",
    "    match_pointclouds(tile_gt, tile_preds)\n",
    "    pred_instances = tile_preds.treeID\n",
    "    pred_instances = format_segmentation_for_PQ(pred_instances)\n",
    "    num, frac = compute_frac_of_gt_found(gt_instances, pred_instances)\n",
    "    num_extra, frac_extra, lst_extra = find_extra_trees(gt_instances, pred_instances)\n",
    "    lst_absolute_val.append(num)\n",
    "    lst_frac.append(frac)\n",
    "    lst_num_extra_trees.append(num_extra)\n",
    "    lst_num_extra_trees_frac.append(frac_extra)\n",
    "    lst_lst_extra_trees.append(lst_extra)\n",
    "print(lst_absolute_val)\n",
    "print(lst_frac)\n",
    "print(lst_num_extra_trees)\n",
    "print(lst_num_extra_trees_frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lst_lst_extra_trees[-1])\n",
    "print(len(lst_lst_extra_trees[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_res = r\"C:\\Users\\swann\\Documents\\EPFL\\Master\\PDM_admin\\medias\\results\\gt\\gt_frac_and_extra_trees_317.png\"\n",
    "#plot results\n",
    "fig, axs = plt.subplots(1,2, figsize=(12,6))\n",
    "sns.lineplot(x=np.array(lst_loops), y=np.array(lst_absolute_val), linewidth=2.5, ax=axs[0], label='pseudo labels')\n",
    "sns.lineplot(x=np.array([lst_loops[0], lst_loops[-1]]), y=np.array([153, 153]), linewidth=2.5, ax=axs[0], label='ground truth')\n",
    "sns.lineplot(x=np.array(lst_loops), y=np.array(lst_num_extra_trees), linewidth=2.5, ax=axs[1])\n",
    "# texts\n",
    "axs[0].text(0.7, 131, f\"131 ({round(lst_frac[0]*100, 2)}%)\", ha='left', va='bottom', fontsize=12, color='black')\n",
    "axs[0].text(6.5,141, f\"140 ({round(lst_frac[-1]*100, 2)}%)\", ha='left', va='bottom', fontsize=12, color='black')\n",
    "axs[0].text(6.5,151.2, f\"153 (100%)\", ha='left', va='bottom', fontsize=12, color='#f67e00')\n",
    "axs[1].text(0.7,27, f\"27 ({round(lst_num_extra_trees_frac[0]*100, 2)}%)\", ha='left', va='bottom', fontsize=12, color='black')\n",
    "axs[1].text(8,40, f\"41 ({round(lst_num_extra_trees_frac[-1]*100, 2)}%)\", ha='center', va='bottom', fontsize=12, color='black')\n",
    "axs[0].set_title('Fraction of gt found')\n",
    "axs[0].set_xlabel('Loop [-]')\n",
    "axs[0].set_ylabel('Frac [%]')\n",
    "axs[1].set_title('Extra trees')\n",
    "axs[1].set_xlabel('Loop [-]')\n",
    "axs[1].set_ylabel('Count [-]')\n",
    "axs[0].grid()\n",
    "axs[1].grid()\n",
    "axs[0].legend(loc=\"center left\")\n",
    "plt.savefig(src_res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot extra_trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_extra_instances = [16, 19, 24, 26, 44, 45, 46, 53, 60, 64, 70, 76, 77, 86, 91, 92, 94, 109, 120, 122, 125, 128, 137, 142, 151, 153, 155, 161, 162, 166, 168, 171, 173, 174, 177, 178, 179, 181, 182, 183, 186]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_layer_extra_trees(data_folder, tile_name, lst_lst_extra_trees= None, loops=None, show_figure=True, save_figure=False):\n",
    "    # security\n",
    "    for loop in loops:\n",
    "        assert os.path.exists(os.path.join(data_folder, str(loop)))\n",
    "\n",
    "    os.makedirs(os.path.join(data_folder, 'extra_trees'), exist_ok=True)\n",
    "    # old_max = 0\n",
    "    for loop in loops:\n",
    "        lst_extra_ids = lst_lst_extra_trees[loop]\n",
    "        loop_tile_src = os.path.join(data_folder, str(loop), 'pseudo_labels', tile_name)\n",
    "        tile = laspy.read(loop_tile_src)\n",
    "        new_dim = np.zeros_like(tile.treeID)\n",
    "        for new_id, tree_id in enumerate(lst_extra_ids):\n",
    "            mask = tile.treeID == tree_id\n",
    "            new_dim[mask] = new_id + 1\n",
    "        # new_dim[tile.treeID > 0] = 1\n",
    "        # new_dim[tile.treeID > old_max] = 2\n",
    "        # old_max = max(tile.treeID)\n",
    "        # add new dim\n",
    "        new_trees = laspy.ExtraBytesParams(name=\"extra_trees\", type=np.uint8)\n",
    "        tile.add_extra_dim(new_trees)\n",
    "        tile['extra_trees'] = new_dim\n",
    "        tile.write(os.path.join(data_folder, 'extra_trees', tile_name.split('.laz')[0] + f'_extra_trees_{loop}.laz'))\n",
    "\n",
    "data_folder = r\"D:\\PDM_repo\\Github\\PDM\\results\\for_paper\\final\\20250701_162429_final_on_gt\"\n",
    "tile_name = \"color_grp_full_tile_317_gt.laz\"\n",
    "add_layer_extra_trees(data_folder, tile_name, lst_lst_extra_trees, loops = range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PDM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
